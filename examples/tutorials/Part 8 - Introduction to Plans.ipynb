{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part 8 - Introduction to Plans.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "pysyft",
      "language": "python",
      "name": "pysyft"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Polarbeargo/PySyft/blob/master/examples/tutorials/Part%208%20-%20Introduction%20to%20Plans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd69aOYU76Xo",
        "colab_type": "text"
      },
      "source": [
        "# Part 8 - Introduction to Plans\n",
        "\n",
        "\n",
        "### Context \n",
        "\n",
        "> Warning: This is still experimental and may change during May / June 2019\n",
        "\n",
        "We introduce here an object which is crucial to scale to industrial Federated Learning: the Plan. It reduces dramatically the bandwidth usage, allows asynchronous schemes and give more autonomy to remote devices. The original concept of plan can be found in the paper [Towards Federated Learning at Scale: System Design](https://arxiv.org/pdf/1902.01046.pdf), but it has been adapted to our needs in the PySyft library using PyTorch.\n",
        "\n",
        "A Plan is intended to store a sequence of torch operations, just like a function, but it allows to send this sequence of operations to remote workers and to keep a reference to it. This way, to compute remotely this sequence of $n$ operations on some remote input referenced through pointers, instead of sending $n$ messages you need now to send a single message with the references of the plan and the pointers. Actually, it's so much like a function that you need a function to build a plan! Hence, for high level users, the notion of plan disappears and is replaced by a magic feature which allow to send to remote workers arbitrary functions containing sequential torch functions.\n",
        "\n",
        "One thing to notice is that the class of functions that you can transform into plans is currently limited to sequences of hooked torch operations exclusively. This excludes in particular logical structures like `if`, `for` and `while` statements, even if we are working to have workarounds soon. _To be completely precise, you can use these but the logical path you take (first `if` to False and 5 loops in `for` for example) in the first computation of your plan will be the one kept for all the next computations, which we want to avoid in the majority of cases._\n",
        "\n",
        "Authors:\n",
        "- Théo Ryffel - Twitter [@theoryffel](https://twitter.com/theoryffel) - GitHub: [@LaRiffle](https://github.com/LaRiffle)\n",
        "- Bobby Wagner - Twitter [@bobbyawagner](https://twitter.com/bobbyawagner) - GitHub: [@robert-wagner](https://github.com/robert-wagner)\n",
        "\t"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhZob00879Yh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1295
        },
        "outputId": "dab0e102-7ca6-414a-d5e6-0feeb3a499f2"
      },
      "source": [
        "\n",
        "!pip install syft"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting syft\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/ad/cd2d1d63f87d69b4764b8bb2898713e638355d654660355b7b7134ca78a8/syft-0.1.17-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from syft) (0.0)\n",
            "Collecting tf-encrypted>=0.5.4 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/f0/b37654fcfe14711509a5d2517b22688f091254491005e4d243f67e726455/tf_encrypted-0.5.4-py3-none-manylinux1_x86_64.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 44.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from syft) (0.3.0)\n",
            "Collecting zstd (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/27/1ea8086d37424e83ab692015cc8dd7d5e37cf791e339633a40dc828dfb74/zstd-1.4.0.0.tar.gz (450kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 43.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack in /usr/local/lib/python3.6/dist-packages (from syft) (0.5.6)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.0)\n",
            "Collecting flask-socketio (from syft)\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/68/fe4806d3a0a5909d274367eb9b3b87262906c1515024f46c2443a36a0c82/Flask_SocketIO-4.1.0-py2.py3-none-any.whl\n",
            "Collecting websockets>=7.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/71/8bfa882b9c502c36e5c9ef6732969533670d2b039cbf95a82ced8f762b80/websockets-7.0-cp36-cp36m-manylinux1_x86_64.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: Flask in /usr/local/lib/python3.6/dist-packages (from syft) (1.0.3)\n",
            "Collecting websocket-client (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 46.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tblib in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.0)\n",
            "Collecting lz4 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/c6/96bbb3525a63ebc53ea700cc7d37ab9045542d33b4d262d0f0408ad9bbf2/lz4-2.1.10-cp36-cp36m-manylinux1_x86_64.whl (385kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 34.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->syft) (0.21.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted>=0.5.4->syft) (1.16.4)\n",
            "Requirement already satisfied: tensorflow<2,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted>=0.5.4->syft) (1.13.1)\n",
            "Collecting pyyaml>=5.1 (from tf-encrypted>=0.5.4->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/837fefac7475963d1eccf4aa684c23b95aa6c1d033a2c5965ccb11e22623/PyYAML-5.1.1.tar.gz (274kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 45.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->syft) (4.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->syft) (1.12.0)\n",
            "Collecting python-socketio>=2.1.0 (from flask-socketio->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/e0/a875abbb0f9d70c6e2ec2019ca9e3893377cef5deca6225ead3497000152/python_socketio-4.1.0-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 1.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask->syft) (7.0)\n",
            "Requirement already satisfied: Werkzeug>=0.14 in /usr/local/lib/python3.6/dist-packages (from Flask->syft) (0.15.4)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask->syft) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.6/dist-packages (from Flask->syft) (2.10.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->syft) (0.13.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->syft) (1.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.13.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.33.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (3.7.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.8.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.2.2)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision->syft) (0.46)\n",
            "Collecting python-engineio>=3.8.0 (from python-socketio>=2.1.0->flask-socketio->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/d3/de0fa7ebebd054de308e6ac397bf9e11ea42924795a38005a21ea001b114/python_engineio-3.8.1-py2.py3-none-any.whl (119kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 46.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10->Flask->syft) (1.1.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (41.0.1)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (3.0.5)\n",
            "Building wheels for collected packages: zstd, pyyaml\n",
            "  Building wheel for zstd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/9a/f4/3105b5209674ac77fcca7fede95184c62a95df0196888e0e76\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/27/a1/775c62ddea7bfa62324fd1f65847ed31c55dadb6051481ba3f\n",
            "Successfully built zstd pyyaml\n",
            "Installing collected packages: pyyaml, tf-encrypted, zstd, python-engineio, python-socketio, flask-socketio, websockets, websocket-client, lz4, syft\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed flask-socketio-4.1.0 lz4-2.1.10 python-engineio-3.8.1 python-socketio-4.1.0 pyyaml-5.1.1 syft-0.1.17 tf-encrypted-0.5.4 websocket-client-0.56.0 websockets-7.0 zstd-1.4.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dLE4xlV76Xq",
        "colab_type": "text"
      },
      "source": [
        "### Imports and model specifications\n",
        "\n",
        "First let's make the official imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zUVTBkj76Xr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ2ePW0i76Xu",
        "colab_type": "text"
      },
      "source": [
        "And than those specific to PySyft"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bO2eI4EK76Xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import syft as sy  # import the Pysyft library\n",
        "hook = sy.TorchHook(torch)  # hook PyTorch ie add extra functionalities \n",
        "server = hook.local_worker"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppG-FY4d76Xx",
        "colab_type": "text"
      },
      "source": [
        "We define remote workers or _devices_, to be consistent with the notions provided in the reference article.\n",
        "We provide them with some data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9aTWtWE76Xx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x11 = torch.tensor([-1, 2.]).tag('input_data')\n",
        "x12 = torch.tensor([1, -2.]).tag('input_data2')\n",
        "x21 = torch.tensor([-1, 2.]).tag('input_data')\n",
        "x22 = torch.tensor([1, -2.]).tag('input_data2')\n",
        "\n",
        "device_1 = sy.VirtualWorker(hook, id=\"device_1\", data=(x11, x12)) \n",
        "device_2 = sy.VirtualWorker(hook, id=\"device_2\", data=(x21, x22))\n",
        "devices = device_1, device_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ-I7xye76Xz",
        "colab_type": "text"
      },
      "source": [
        "### Basic example\n",
        "\n",
        "Let's define a function that we want to transform into a plan. To do so, it's as simple as adding a decorator above the function definition!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIcBILv-76X0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@sy.func2plan\n",
        "def plan_double_abs(x):\n",
        "    x = x + x\n",
        "    x = torch.abs(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baI3SES-76X2",
        "colab_type": "text"
      },
      "source": [
        "Let's check, yes we have now a plan!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2jk1I6676X3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3a0cec0-d533-4d51-b66c-ab4794c9354e"
      },
      "source": [
        "plan_double_abs"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<syft.federated.plan.Plan at 0x7f0deb694588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxTczoDA76X7",
        "colab_type": "text"
      },
      "source": [
        "To use a plan, you need two things: to build the plan (_ie register the sequence of operations present in the function_) and to send it to a worker / device. Fortunately you can do this very easily!\n",
        "\n",
        "We first get a reference to some remote data: a request is sent over the network and a reference pointer is returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkQcqSFK76X8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f8e56b2f-cb54-4c32-b8dc-6077f544cdb6"
      },
      "source": [
        "pointer_to_data = device_1.search('input_data')[0]\n",
        "pointer_to_data"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Wrapper)>[PointerTensor | me:19489904171 -> device_1:76367417931]\n",
              "\tTags: input_data \n",
              "\tShape: torch.Size([2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whK4l92E76X_",
        "colab_type": "text"
      },
      "source": [
        "We tell the plan it must be executed remotely on the device, but actually nothing happens on the network because we have not provided any input data! You can now observe that there is an attribute location specified `location:device_1`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZolNx_g76YA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4d62293-d503-4040-dfa5-d46c2585fa8d"
      },
      "source": [
        "plan_double_abs.send(device_1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<syft.federated.plan.Plan at 0x7f0deb694588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyqBbUud76YD",
        "colab_type": "text"
      },
      "source": [
        "One important thing to remember is now that we pre-set ahead of computation the id(s) where the result(s) should be stored. This will allow to send commands asynchronously, to already have a reference to a virtual result and to continue local computations without waiting for the remote result to be computed. One major application is when you require computation of a batch on device_1 and don't want to wait for this computation to end to launch another batch computation on device_2.\n",
        "\n",
        "We now feed the plan with a reference pointer to some data. Three things happens: (1) the plan is built, (2) it is sent to the device and (3) it is run remotely.\n",
        "1. All the commands are executed sequentially by the local worker, and instead of being sent to the device they are catched by the plan and stored in his ... plan attribute!\n",
        "2. This newly plan object is sent to the remote worker in a single communication round.\n",
        "3. An other command is issued to run this plan remotely, so that the predefined location of the output of the plan now contains the result (remember we pre-set location of result ahead of computation). This also require a single communication round. _That's the place where we could run asynchronously_.\n",
        "\n",
        "The result is simply a pointer, just like when you call an usual hooked torch function!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FxIZ5FF76YD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "45ce0416-4db1-4bc7-8576-62ded7616546"
      },
      "source": [
        "%%time\n",
        "# %%time is a Magic comand to log a cell's execution time\n",
        "\n",
        "pointer_to_result = plan_double_abs(pointer_to_data)\n",
        "print(pointer_to_result)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[PointerTensor | me:43019130603 -> device_1:6265600755]\n",
            "CPU times: user 113 ms, sys: 206 µs, total: 113 ms\n",
            "Wall time: 154 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZxm1WKZ76YG",
        "colab_type": "text"
      },
      "source": [
        "And you can simply ask the value back."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY1xPHQ876YH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75809e52-0736-41cd-b01a-718255a9d705"
      },
      "source": [
        "pointer_to_result.get()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 4.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtfH3Ddt76YK",
        "colab_type": "text"
      },
      "source": [
        "What happen now if you ask a second computation with this plan?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNZ7DDHt76YL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pointer_to_data = device_1.search('input_data2')[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNvWU4mi76YO",
        "colab_type": "text"
      },
      "source": [
        "Now it is much faster because there is a single communication round: we already have a reference to the remote plan, so we can require a remote execution and just provide the reference location of the new remote inputs. For the end user, nothing changes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FVF1lxJ76YP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0c9f78f3-380d-45f2-88db-9f2decf19799"
      },
      "source": [
        "%time\n",
        "pointer_to_result = plan_double_abs(pointer_to_data)\n",
        "print(pointer_to_result.get())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 8.11 µs\n",
            "tensor([2., 4.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vqRH6fF76YS",
        "colab_type": "text"
      },
      "source": [
        "### Towards a concrete example\n",
        "\n",
        "But what we want to do is to apply Plan to Deep and Federated Learning, right? So let's look to a slightly more complicated example, using neural networks as you might be willing to use them.\n",
        "Note that we are now transforming a method into a plan, so we use the `@` `sy.meth2plan` decorator instead"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMpvsfjj76YT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 3)\n",
        "        self.fc2 = nn.Linear(3, 2)\n",
        "\n",
        "    @sy.method2plan\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuXzIOpZ76YV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = Net()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS0qQXAB76YY",
        "colab_type": "text"
      },
      "source": [
        "So the only thing we did is adding the sy.method2plan decorator! And check that `net.forward` is again a plan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-3T4DF876YZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30ff3bba-ee3d-4ff2-94ee-e6aa69bb90e7"
      },
      "source": [
        "net.forward"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<syft.federated.plan.Plan at 0x7f0deb69fcc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICbdO0Tc76Ye",
        "colab_type": "text"
      },
      "source": [
        "Now there is a subtlety: because the plan depend on the net instance, if you send the plan *you also need to send the model*.\n",
        "\n",
        "> For developers: this is not compulsory as you actually have a reference to the model in the plan, we could call model.send internally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8CJ7Wfb76Yf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7a7d554a-3d20-453e-bdae-3dd3e8e7ee4c"
      },
      "source": [
        "net.send(device_1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (fc1): Linear(in_features=2, out_features=3, bias=True)\n",
              "  (fc2): Linear(in_features=3, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU5S-uwx76Yl",
        "colab_type": "text"
      },
      "source": [
        "Let's retrieve some remote data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoBlMOP-76Ym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pointer_to_data = device_1.search('input_data')[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVRBLV-O76Yo",
        "colab_type": "text"
      },
      "source": [
        "Then, the syntax is just like normal remote sequential execution, that is, just like local execution. But compared to classic remote execution, there is a single communication round for each execution (except the first time where, as described above, we first build and send the plan)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmE6Im7W76Yo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23223b7b-d1dc-4059-f135-b66c08fdac64"
      },
      "source": [
        "pointer_to_result = net(pointer_to_data)\n",
        "pointer_to_result"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PointerTensor | me:84150659728 -> device_1:93711020065]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNqemwaO76Yw",
        "colab_type": "text"
      },
      "source": [
        "And we get the result as usual!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4LCNJf476Yw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d0ebfd6-54d7-449b-904e-9b0a84e3b02f"
      },
      "source": [
        "pointer_to_result.get()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.9984, -0.4596], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh4MnMXJ76Y0",
        "colab_type": "text"
      },
      "source": [
        "Et voilà! We have seen how to dramatically reduce the communication between the local worker (or server) and the remote devices!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DorZoyqZ76Y1",
        "colab_type": "text"
      },
      "source": [
        "### Switch between workers\n",
        "\n",
        "One major feature that we want to have is to use the same plan for several workers, that we would change depending on the remote batch of data we are considering.\n",
        "In particular, we don't want to rebuild the plan each time we change of worker. Let's see how we do this, using the previous example with our small network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlEKVQKN76Y1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 3)\n",
        "        self.fc2 = nn.Linear(3, 2)\n",
        "\n",
        "    @sy.method2plan\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0sMu10076Y4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = Net()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aniAwOLY76Y6",
        "colab_type": "text"
      },
      "source": [
        "Here are the main steps we just executed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3_EXw_O76Y6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe3a5a9e-d6c7-4d69-8f42-91d0597b06f0"
      },
      "source": [
        "net.send(device_1)\n",
        "pointer_to_data = device_1.search('input_data')[0]\n",
        "pointer_to_result = net(pointer_to_data)\n",
        "pointer_to_result.get()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.3326, -0.3062], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGp3z0EY76Y9",
        "colab_type": "text"
      },
      "source": [
        "Let's get the model and the network back"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMbQ9SX876Y-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ccd9a423-ed5c-475a-e4b6-e0b425ab16de"
      },
      "source": [
        "net.get()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (fc1): Linear(in_features=2, out_features=3, bias=True)\n",
              "  (fc2): Linear(in_features=3, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL21t6Tm76ZA",
        "colab_type": "text"
      },
      "source": [
        "And actually the syntax is straight forward: we just send it to another device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "ciRKufxm76ZA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57daac33-0101-4e8e-bcb5-3d9fc9a27264"
      },
      "source": [
        "net.send(device_2)\n",
        "pointer_to_data = device_2.search('input_data')[0]\n",
        "pointer_to_result = net(pointer_to_data)\n",
        "pointer_to_result.get()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.3326, -0.3062], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1a7h9st76ZD",
        "colab_type": "text"
      },
      "source": [
        "At this point, the interest of Plans might not be straightforward, but we will soon reuse all these elements for a larger scale Federated Learning task where we will need more complex interactions between workers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV8Qft1L76ZD",
        "colab_type": "text"
      },
      "source": [
        "### Star PySyft on GitHub\n",
        "\n",
        "The easiest way to help our community is just by starring the repositories! This helps raise awareness of the cool tools we're building.\n",
        "\n",
        "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
        "\n",
        "### Pick our tutorials on GitHub!\n",
        "\n",
        "We made really nice tutorials to get a better understanding of what Federated and Privacy-Preserving Learning should look like and how we are building the bricks for this to happen.\n",
        "\n",
        "- [Checkout the PySyft tutorials](https://github.com/OpenMined/PySyft/tree/master/examples/tutorials)\n",
        "\n",
        "\n",
        "### Join our Slack!\n",
        "\n",
        "The best way to keep up to date on the latest advancements is to join our community! \n",
        "\n",
        "- [Join slack.openmined.org](http://slack.openmined.org)\n",
        "\n",
        "### Join a Code Project!\n",
        "\n",
        "The best way to contribute to our community is to become a code contributor! If you want to start \"one off\" mini-projects, you can go to PySyft GitHub Issues page and search for issues marked `Good First Issue`.\n",
        "\n",
        "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
        "\n",
        "### Donate\n",
        "\n",
        "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
        "\n",
        "- [Donate through OpenMined's Open Collective Page](https://opencollective.com/openmined)"
      ]
    }
  ]
}