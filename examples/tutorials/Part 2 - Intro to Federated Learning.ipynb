{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part 2 - Intro to Federated Learning.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3maVkVjYbzl",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: Intro to Federated Learning\n",
        "\n",
        "In the last section, we learned about PointerTensors, which create the underlying infrastructure we need for privacy preserving Deep Learning. In this section, we're going to see how to use these basic tools to implement our first privacy preserving deep learning algorithm, Federated Learning.\n",
        "\n",
        "Authors:\n",
        "- Andrew Trask - Twitter: [@iamtrask](https://twitter.com/iamtrask)\n",
        "\n",
        "### What is Federated Learning?\n",
        "\n",
        "It's a simple, powerful way to train Deep Learning models. If you think about training data, it's always the result of some sort of collection process. People (via devices) generate data by recording events in the real world. Normally, this data is aggregated to a single, central location so that you can train a machine learning model. Federated Learning turns this on its head!\n",
        "\n",
        "Instead of bringing training data to the model (a central server), you bring the model to the training data (wherever it may live).\n",
        "\n",
        "The idea is that this allows whoever is creating the data to own the only permanent copy, and thus maintain control over who ever has access to it. Pretty cool, eh?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX1_MrymYbzo",
        "colab_type": "text"
      },
      "source": [
        "# Section 2.1 - A Toy Federated Learning Example\n",
        "\n",
        "Let's start by training a toy model the centralized way. This is about a simple as models get. We first need:\n",
        "\n",
        "- a toy dataset\n",
        "- a model\n",
        "- some basic training logic for training a model to fit the data.\n",
        "\n",
        "Note: If this API is un-familiar to you - head on over to [fast.ai](http://fast.ai) and take their course before continuing in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB10fhYwYbzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTc-a11cYbzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# A Toy Dataset\n",
        "data = torch.tensor([[0,0],[0,1],[1,0],[1,1.]], requires_grad=True)\n",
        "target = torch.tensor([[0],[0],[1],[1.]], requires_grad=True)\n",
        "\n",
        "# A Toy Model\n",
        "model = nn.Linear(2,1)\n",
        "\n",
        "def train():\n",
        "    # Training Logic\n",
        "    opt = optim.SGD(params=model.parameters(),lr=0.1)\n",
        "    for iter in range(20):\n",
        "\n",
        "        # 1) erase previous gradients (if they exist)\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # 2) make a prediction\n",
        "        pred = model(data)\n",
        "\n",
        "        # 3) calculate how much we missed\n",
        "        loss = ((pred - target)**2).sum()\n",
        "\n",
        "        # 4) figure out which weights caused us to miss\n",
        "        loss.backward()\n",
        "\n",
        "        # 5) change those weights\n",
        "        opt.step()\n",
        "\n",
        "        # 6) print our progress\n",
        "        print(loss.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZVrgRkzYbzu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "b336d959-2a7d-4823-e437-c5bd36d166c1"
      },
      "source": [
        "train()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.7879)\n",
            "tensor(0.6441)\n",
            "tensor(0.4126)\n",
            "tensor(0.2932)\n",
            "tensor(0.2121)\n",
            "tensor(0.1546)\n",
            "tensor(0.1134)\n",
            "tensor(0.0836)\n",
            "tensor(0.0620)\n",
            "tensor(0.0461)\n",
            "tensor(0.0344)\n",
            "tensor(0.0258)\n",
            "tensor(0.0194)\n",
            "tensor(0.0146)\n",
            "tensor(0.0111)\n",
            "tensor(0.0084)\n",
            "tensor(0.0063)\n",
            "tensor(0.0048)\n",
            "tensor(0.0037)\n",
            "tensor(0.0028)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNcdfbCZYbz0",
        "colab_type": "text"
      },
      "source": [
        "And there you have it! We've trained a basic model in the conventional manner. All our data is aggregated into our local machine and we can use it to make updates to our model. Federated Learning, however, doesn't work this way. So, let's modify this example to do it the Federated Learning way! \n",
        "\n",
        "So, what do we need:\n",
        "\n",
        "- create a couple workers\n",
        "- get pointers to training data on each worker\n",
        "- updated training logic to do federated learning\n",
        "\n",
        "    New Training Steps:\n",
        "    - send model to correct worker\n",
        "    - train on the data located there\n",
        "    - get the model back and repeat with next worker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhmbLq-sY7wS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1295
        },
        "outputId": "15c80bdb-fe52-4bca-b319-fd0a2d1fae28"
      },
      "source": [
        "\n",
        "!pip install syft"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting syft\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/ad/cd2d1d63f87d69b4764b8bb2898713e638355d654660355b7b7134ca78a8/syft-0.1.17-py3-none-any.whl (176kB)\n",
            "\r\u001b[K     |█▉                              | 10kB 15.4MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51kB 3.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 81kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 92kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 102kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 112kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 122kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 133kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 143kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 153kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 163kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 174kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from syft) (0.3.0)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from syft) (0.0)\n",
            "Collecting websocket-client (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 46.3MB/s \n",
            "\u001b[?25hCollecting websockets>=7.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/71/8bfa882b9c502c36e5c9ef6732969533670d2b039cbf95a82ced8f762b80/websockets-7.0-cp36-cp36m-manylinux1_x86_64.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 26.7MB/s \n",
            "\u001b[?25hCollecting zstd (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/27/1ea8086d37424e83ab692015cc8dd7d5e37cf791e339633a40dc828dfb74/zstd-1.4.0.0.tar.gz (450kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 54.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: Flask in /usr/local/lib/python3.6/dist-packages (from syft) (1.0.3)\n",
            "Collecting lz4 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/c6/96bbb3525a63ebc53ea700cc7d37ab9045542d33b4d262d0f0408ad9bbf2/lz4-2.1.10-cp36-cp36m-manylinux1_x86_64.whl (385kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 35.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tblib in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.0)\n",
            "Collecting flask-socketio (from syft)\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/68/fe4806d3a0a5909d274367eb9b3b87262906c1515024f46c2443a36a0c82/Flask_SocketIO-4.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.6/dist-packages (from syft) (0.5.6)\n",
            "Collecting tf-encrypted>=0.5.4 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/f0/b37654fcfe14711509a5d2517b22688f091254491005e4d243f67e726455/tf_encrypted-0.5.4-py3-none-manylinux1_x86_64.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 55.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->syft) (4.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->syft) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision->syft) (1.16.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->syft) (0.21.2)\n",
            "Requirement already satisfied: Werkzeug>=0.14 in /usr/local/lib/python3.6/dist-packages (from Flask->syft) (0.15.4)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask->syft) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.6/dist-packages (from Flask->syft) (2.10.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask->syft) (7.0)\n",
            "Collecting python-socketio>=2.1.0 (from flask-socketio->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/e0/a875abbb0f9d70c6e2ec2019ca9e3893377cef5deca6225ead3497000152/python_socketio-4.1.0-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 15.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted>=0.5.4->syft) (1.13.1)\n",
            "Collecting pyyaml>=5.1 (from tf-encrypted>=0.5.4->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/837fefac7475963d1eccf4aa684c23b95aa6c1d033a2c5965ccb11e22623/PyYAML-5.1.1.tar.gz (274kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 53.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision->syft) (0.46)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->syft) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->syft) (0.13.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10->Flask->syft) (1.1.1)\n",
            "Collecting python-engineio>=3.8.0 (from python-socketio>=2.1.0->flask-socketio->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/d3/de0fa7ebebd054de308e6ac397bf9e11ea42924795a38005a21ea001b114/python_engineio-3.8.1-py2.py3-none-any.whl (119kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 47.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.13.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.33.4)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (3.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.7.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.8.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.13.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.15.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (3.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (2.8.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (3.0.5)\n",
            "Building wheels for collected packages: zstd, pyyaml\n",
            "  Building wheel for zstd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/9a/f4/3105b5209674ac77fcca7fede95184c62a95df0196888e0e76\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/27/a1/775c62ddea7bfa62324fd1f65847ed31c55dadb6051481ba3f\n",
            "Successfully built zstd pyyaml\n",
            "Installing collected packages: websocket-client, websockets, zstd, lz4, python-engineio, python-socketio, flask-socketio, pyyaml, tf-encrypted, syft\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed flask-socketio-4.1.0 lz4-2.1.10 python-engineio-3.8.1 python-socketio-4.1.0 pyyaml-5.1.1 syft-0.1.17 tf-encrypted-0.5.4 websocket-client-0.56.0 websockets-7.0 zstd-1.4.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qvzgw6A_Ybz1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import syft as sy\n",
        "hook = sy.TorchHook(torch)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKg0wcXgYbz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a couple workers\n",
        "\n",
        "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
        "alice = sy.VirtualWorker(hook, id=\"alice\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSvSozb8Ybz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A Toy Dataset\n",
        "data = torch.tensor([[0,0],[0,1],[1,0],[1,1.]], requires_grad=True)\n",
        "target = torch.tensor([[0],[0],[1],[1.]], requires_grad=True)\n",
        "\n",
        "# get pointers to training data on each worker by\n",
        "# sending some training data to bob and alice\n",
        "data_bob = data[0:2]\n",
        "target_bob = target[0:2]\n",
        "\n",
        "data_alice = data[2:]\n",
        "target_alice = target[2:]\n",
        "\n",
        "# Iniitalize A Toy Model\n",
        "model = nn.Linear(2,1)\n",
        "\n",
        "data_bob = data_bob.send(bob)\n",
        "data_alice = data_alice.send(alice)\n",
        "target_bob = target_bob.send(bob)\n",
        "target_alice = target_alice.send(alice)\n",
        "\n",
        "# organize pointers into a list\n",
        "datasets = [(data_bob,target_bob),(data_alice,target_alice)]\n",
        "\n",
        "opt = optim.SGD(params=model.parameters(),lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU5N4IJ8Ybz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    # Training Logic\n",
        "    opt = optim.SGD(params=model.parameters(),lr=0.1)\n",
        "    for iter in range(10):\n",
        "        \n",
        "        # NEW) iterate through each worker's dataset\n",
        "        for data,target in datasets:\n",
        "            \n",
        "            # NEW) send model to correct worker\n",
        "            model.send(data.location)\n",
        "\n",
        "            # 1) erase previous gradients (if they exist)\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # 2) make a prediction\n",
        "            pred = model(data)\n",
        "\n",
        "            # 3) calculate how much we missed\n",
        "            loss = ((pred - target)**2).sum()\n",
        "\n",
        "            # 4) figure out which weights caused us to miss\n",
        "            loss.backward()\n",
        "\n",
        "            # 5) change those weights\n",
        "            opt.step()\n",
        "            \n",
        "            # NEW) get model (with gradients)\n",
        "            model.get()\n",
        "\n",
        "            # 6) print our progress\n",
        "            print(loss.get()) # NEW) slight edit... need to call .get() on loss\\\n",
        "    \n",
        "# federated averaging"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0m6w05-Yb0A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "4c9633b4-ea0d-42c4-ec36-f0df6ecff4f0"
      },
      "source": [
        "train()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.6169, requires_grad=True)\n",
            "tensor(3.8155, requires_grad=True)\n",
            "tensor(0.1905, requires_grad=True)\n",
            "tensor(0.1517, requires_grad=True)\n",
            "tensor(0.1771, requires_grad=True)\n",
            "tensor(0.0612, requires_grad=True)\n",
            "tensor(0.1161, requires_grad=True)\n",
            "tensor(0.0379, requires_grad=True)\n",
            "tensor(0.0754, requires_grad=True)\n",
            "tensor(0.0247, requires_grad=True)\n",
            "tensor(0.0495, requires_grad=True)\n",
            "tensor(0.0165, requires_grad=True)\n",
            "tensor(0.0329, requires_grad=True)\n",
            "tensor(0.0112, requires_grad=True)\n",
            "tensor(0.0221, requires_grad=True)\n",
            "tensor(0.0077, requires_grad=True)\n",
            "tensor(0.0151, requires_grad=True)\n",
            "tensor(0.0054, requires_grad=True)\n",
            "tensor(0.0104, requires_grad=True)\n",
            "tensor(0.0039, requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj27gwTwYb0D",
        "colab_type": "text"
      },
      "source": [
        "## Well Done!\n",
        "\n",
        "And voilà! We now are training a very simple Deep Learning model using Federated Learning! We send the model to each worker, generate a new gradient, and then bring the gradient back to our local server where we update our global model. Never in this process do we ever see or request access to the underlying training data! We preserve the privacy of Bob and Alice!!!\n",
        "\n",
        "## Shortcomings of this Example\n",
        "\n",
        "So, while this example is a nice introduction to Federated Learning, it still has some major shortcomings. Most notably, when we call `model.get()` and receive the updated model from Bob or Alice, we can actually learn a lot about Bob and Alice's training data by looking at their gradients. In some cases, we can restore their training data perfectly! \n",
        "\n",
        "So, what is there to do? Well, the first strategy people employ is to **average the gradient across multiple individuals before uploading it to the central server**. This strategy, however, will require some more sophisticated use of PointerTenor objects. So, in the next section, we're going to take some time to learn about more advanced pointer functionality and then we'll upgrade this Federated Learning example\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQyMxhwrYb0E",
        "colab_type": "text"
      },
      "source": [
        "# Congratulations!!! - Time to Join the Community!\n",
        "\n",
        "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!\n",
        "\n",
        "### Star PySyft on GitHub\n",
        "\n",
        "The easiest way to help our community is just by starring the Repos! This helps raise awareness of the cool tools we're building.\n",
        "\n",
        "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
        "\n",
        "### Join our Slack!\n",
        "\n",
        "The best way to keep up to date on the latest advancements is to join our community! You can do so by filling out the form at [http://slack.openmined.org](http://slack.openmined.org)\n",
        "\n",
        "### Join a Code Project!\n",
        "\n",
        "The best way to contribute to our community is to become a code contributor! At any time you can go to PySyft GitHub Issues page and filter for \"Projects\". This will show you all the top level Tickets giving an overview of what projects you can join! If you don't want to join a project, but you would like to do a bit of coding, you can also look for more \"one off\" mini-projects by searching for GitHub issues marked \"good first issue\".\n",
        "\n",
        "- [PySyft Projects](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
        "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
        "\n",
        "### Donate\n",
        "\n",
        "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
        "\n",
        "[OpenMined's Open Collective Page](https://opencollective.com/openmined)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LOT-4ioYb0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpUUSt-BYb0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}